{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Carregar dataset de tradução (Português → Inglês)\n",
        "examples, metadata = tfds.load(\"ted_hrlr_translate/pt_to_en\", as_supervised=True, with_info=True)\n",
        "train_data, val_data = examples[\"train\"], examples[\"validation\"]"
      ],
      "metadata": {
        "id": "cmCGhNIlSz_i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Criar dataset de frases traduzidas\n",
        "frases_pt = [\"Olá, como vai?\", \"Esse é um teste.\", \"Estamos aprendendo Transformers.\"]\n",
        "frases_en = [\"Hello, how are you?\", \"This is a test.\", \"We are learning Transformers.\"]\n",
        "\n",
        "# Converter para TensorFlow Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((frases_pt, frases_en))\n",
        "\n",
        "# Exibir exemplos do dataset\n",
        "for pt, en in dataset.take(3):\n",
        "    print(pt.numpy(), \"->\", en.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iznmyuMdS23s",
        "outputId": "e2d866f6-4845-4da4-fe66-91f3a08e0b0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Ol\\xc3\\xa1, como vai?' -> b'Hello, how are you?'\n",
            "b'Esse \\xc3\\xa9 um teste.' -> b'This is a test.'\n",
            "b'Estamos aprendendo Transformers.' -> b'We are learning Transformers.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VQwTvpS_Oj24"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "  dataset_name = \"ted_hrlr_translate/pt_to_en\"\n",
        "  examples, metadata = tfds.load(dataset_name, as_supervised=True, with_info=True)\n",
        "  return examples[\"train\"], examples[\"validation\"]\n",
        "\n",
        "def tokenize_text(text, tokenizer):\n",
        "  return tokenizer.encode(text.numpy())\n",
        "\n",
        "def encode_map_fn(pt, en, tokenizer_pt, tokenizer_en):\n",
        "  return tf.convert_to_tensor(tokenize_text(pt, tokenizer_pt), dtype=tf.int64), tf.convert_to_tensor(tokenize_text(en, tokenizer_en), dtype=tf.int64)\n",
        "\n",
        "def prepare_data():\n",
        "  train_data, val_data = load_data()\n",
        "  tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "  (pt.numpy() for pt, en in train_data), target_vocab_size=213)\n",
        "  tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "  (en.numpy() for pt, en in train_data), target_vocab_size=213)\n",
        "  train_data = train_data.map(lambda pt, en: encode_map_fn(pt, en, tokenizer_pt, tokenizer_en))\n",
        "  val_data = val_data.map(lambda pt, en: encode_map_fn(pt, en, tokenizer_pt, tokenizer_en))\n",
        "  return train_data, val_data, tokenizer_pt, tokenizer_en"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    depth = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output"
      ],
      "metadata": {
        "id": "z-JXYZ1hPbIn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def init(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, rate=0.1):\n",
        "    super(Transformer, self).init()\n",
        "    self.encoder = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.decoder = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "      inputs = tf.expand_dims(inputs, axis=-1)  # Adiciona uma dimensão extra\n",
        "      enc_output = self.encoder(inputs)\n",
        "      dec_output = self.decoder(inputs)\n",
        "      final_output = self.final_layer(dec_output)\n",
        "      return final_output"
      ],
      "metadata": {
        "id": "xCI073mfPquP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, epochs=10):\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer=optimizer, loss=loss_object, metrics=[\"accuracy\"])\n",
        "  model.fit(dataset, epochs=epochs)\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "  loss, acc = model.evaluate(dataset)\n",
        "  print(f\"Loss: {loss}, Accuracy: {acc}\")\n",
        "\n",
        "  if name == \"main\":\n",
        "    train_data, val_data, tokenizer_pt, tokenizer_en = prepare_data()\n",
        "    transformer = Transformer(num_layers=4, d_model=128, num_heads=8, dff=512, input_vocab_size=8000, target_vocab_size=8000)\n",
        "    train_model(transformer, train_data)\n",
        "    evaluate_model(transformer, val_data)"
      ],
      "metadata": {
        "id": "FmEmsKOtTZUS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... (rest of your code, including load_data, encode_map_fn, prepare_data, scaled_dot_product_attention) ...\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.decoder = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # inputs should be a tuple of (pt, en)\n",
        "        enc_output = self.encoder(inputs[0])\n",
        "        dec_output = self.decoder(inputs[1])\n",
        "\n",
        "        # Apply the final layer to the decoder output\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output\n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs=10):\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss_object, metrics=[\"accuracy\"])\n",
        "    history = model.fit(dataset, epochs=epochs)\n",
        "    return history\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "    loss, acc = model.evaluate(dataset)\n",
        "    print(f\"Loss: {loss}, Accuracy: {acc}\")\n",
        "    return loss, acc\n",
        "\n",
        "def plot_results(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Loss')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Evolução da Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Evolução da Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_data, val_data, tokenizer_pt, tokenizer_en = prepare_data()\n",
        "\n",
        "    # Explicitly provide the input shape to the model\n",
        "    transformer = Transformer(num_layers=4, d_model=128, num_heads=8, dff=512,\n",
        "                              input_vocab_size=8000, target_vocab_size=8000)\n",
        "\n",
        "    history = train_model(transformer, train_data)\n",
        "    loss, acc = evaluate_model(transformer, val_data)\n",
        "    plot_results(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5K-K8Wk7T4x9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}